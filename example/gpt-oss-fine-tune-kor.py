from transformers import TextStreamer
from unsloth import FastLanguageModel
import torch

max_seq_length = 1024
dtype = None

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/gpt-oss-20b-unsloth-bnb-4bit", # 20B model using bitsandbytes 4bit quantization
    "unsloth/gpt-oss-120b-unsloth-bnb-4bit",
    "unsloth/gpt-oss-20b", # 20B model using MXFP4 format
    "unsloth/gpt-oss-120b",
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/gpt-oss-20b-unsloth-bnb-4bit",
    dtype = dtype, # None for auto detection
    max_seq_length = max_seq_length, # Choose any for long context!
    load_in_4bit = True, # 4 bit quantization to reduce memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = "hf_...", # use one if using gated models
)
model = FastLanguageModel.get_peft_model(
    model,
    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

messages = [
    {"role": "user", "content": "Solve x^5 + 3x^4 - 10 = 3."},
]
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt = True,
    return_tensors = "pt",
    return_dict = True,
    reasoning_effort = "medium", # **NEW!** Set reasoning effort to low, medium or high
).to(model.device)

_ = model.generate(**inputs, max_new_tokens = 128, streamer = TextStreamer(tokenizer),
                   temperature = 1.0, top_p = 1.0)



def formatting_prompts_func(examples):
    convos = examples["messages"]
    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]
    return { "text" : texts, }
pass

from datasets import load_dataset
dataset = load_dataset("maywell/korean_textbooks", name="claude_evol", split="train")

def formatting_prompts_func(examples):
    # ì´ë¯¸ ì™„ì„±ëœ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    texts = examples["text"]
    return {"text": texts}


print(dataset[0]['text'])



from trl import SFTConfig, SFTTrainer
trainer = SFTTrainer(
    model = model,
    processing_class = tokenizer,
    train_dataset = dataset,
    args = SFTConfig(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        # num_train_epochs = 1, # Set this for 1 full training run.
        max_steps = 30,
        learning_rate = 2e-4,
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc
    ),
)


# @title í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")


trainer_stats = trainer.train()


# @title ìµœì¢… ë©”ëª¨ë¦¬ ë° ì‹œê°„ í†µê³„ í™•ì¸
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(
    f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training."
)
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")


messages = [
    {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ êµìœ¡ ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ” ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
    {"role": "user", "content": "2ì˜ ê±°ë“­ì œê³±ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True,
    reasoning_effort="medium",
).to(model.device)

from transformers import TextStreamer
_ = model.generate(
    **inputs,
    max_new_tokens=512,  # 128ì—ì„œ 512ë¡œ ì¦ê°€
    streamer=TextStreamer(tokenizer),
    do_sample=True,      # ë” ë‹¤ì–‘í•œ ì‘ë‹µì„ ìœ„í•´ ì¶”ê°€
    temperature=0.7,     # ì‘ë‹µì˜ ì°½ì˜ì„± ì¡°ì ˆ
    pad_token_id=tokenizer.eos_token_id  # íŒ¨ë”© í† í° ì„¤ì •
)

# ì‹¬í”Œí•œ íŒŒì¸íŠœë‹ ëª¨ë¸ ì—…ë¡œë“œ ì½”ë“œ

import os
import json
import shutil
from huggingface_hub import HfApi, login

def fix_adapter_config(model_path):
    """adapter_config.json ìˆ˜ì • (ì—ëŸ¬ ë°©ì§€)"""
    config_path = os.path.join(model_path, "adapter_config.json")

    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)

        # ë¬¸ìì—´ë¡œ ë³€í™˜ (ì—ëŸ¬ í•´ê²°)
        config['task_type'] = "CAUSAL_LM"
        config['peft_type'] = "LORA"

        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)

        print("âœ… adapter_config.json ìˆ˜ì • ì™„ë£Œ")

def create_model_card(model_path, repo_name):
    """í•œêµ­ì–´ ëª¨ë¸ì¹´ë“œ ìƒì„±"""

    readme_content = f"""---
license: apache-2.0
base_model: unsloth/gpt-oss-20b
tags:
- unsloth
- lora
- korean
- education
- textbook
- gpt-oss
- í•œêµ­ì–´
- êµìœ¡
- íŒŒì¸íŠœë‹
language:
- ko
datasets:
- maywell/korean_textbooks
library_name: peft
pipeline_tag: text-generation
---

# í•œêµ­ì–´ êµìœ¡ ìë£Œ íŒŒì¸íŠœë‹ ëª¨ë¸ (Korean Textbook Fine-tuned Model)

## ğŸ“š ëª¨ë¸ ì†Œê°œ

ì´ ëª¨ë¸ì€ **unsloth/gpt-oss-20b**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **maywell/korean_textbooks** ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ëœ í•œêµ­ì–´ êµìœ¡ ì „ìš© ëª¨ë¸ì…ë‹ˆë‹¤.
LoRA(Low-Rank Adaptation) ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìœ¼ë©°, í•œêµ­ì–´ êµìœ¡ ì½˜í…ì¸  ìƒì„±ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” íŠ¹ì§•

- **ë² ì´ìŠ¤ ëª¨ë¸**: unsloth/gpt-oss-20b (20B íŒŒë¼ë¯¸í„°)
- **í›ˆë ¨ ë°©ë²•**: LoRA (Low-Rank Adaptation)
- **íŠ¹í™” ë¶„ì•¼**: í•œêµ­ì–´ êµìœ¡ ì½˜í…ì¸  ìƒì„±
- **ë°ì´í„°ì…‹**: maywell/korean_textbooks
- **ì–¸ì–´**: í•œêµ­ì–´ (Korean)

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ëª¨ë¸ ë¡œë“œ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    "unsloth/gpt-oss-20b",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# LoRA ì–´ëŒ‘í„° ë¡œë“œ
model = PeftModel.from_pretrained(base_model, "{repo_name}")

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("{repo_name}")
```

### ì‚¬ìš© ì˜ˆì‹œ

```python
messages = [
    {{"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ êµìœ¡ ë‚´ìš©ì„ ì„¤ëª…í•˜ëŠ” ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."}},
    {{"role": "user", "content": "2ì˜ ê±°ë“­ì œê³±ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."}}
]

inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True
).to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )

response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)
```

## ğŸ“Š í›ˆë ¨ ì •ë³´

- **ë² ì´ìŠ¤ ëª¨ë¸**: unsloth/gpt-oss-20b-unsloth-bnb-4bit
- **í›ˆë ¨ ìŠ¤í…**: 30 steps
- **LoRA Rank**: 8
- **LoRA Alpha**: 16
- **íƒ€ê²Ÿ ëª¨ë“ˆ**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **ë°ì´í„°ì…‹**: maywell/korean_textbooks

## ğŸ“ í™œìš© ë¶„ì•¼

ì´ ëª¨ë¸ì€ ë‹¤ìŒ ë¶„ì•¼ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤:

### ìˆ˜í•™ (Mathematics)
- ê¸°ì´ˆ ìˆ˜í•™ ê°œë… ì„¤ëª…
- ëŒ€ìˆ˜, ê¸°í•˜, ë¯¸ì ë¶„ ë¬¸ì œ í•´ì„¤
- ìˆ˜í•™ ê³µì‹ì˜ ì§ê´€ì  ì´í•´

### ê³¼í•™ (Science)
- ë¬¼ë¦¬, í™”í•™, ìƒë¬¼í•™ ì›ë¦¬ ì„¤ëª…
- ì‹¤í—˜ ê³¼ì • ë° ê²°ê³¼ í•´ì„
- ê³¼í•™ì  í˜„ìƒì˜ ì´í•´

### ì–¸ì–´ (Language)
- í•œêµ­ì–´ ë¬¸ë²• ë° ì–´íœ˜ ì„¤ëª…
- ë¬¸í•™ ì‘í’ˆ ë¶„ì„ ë° í•´ì„
- ê¸€ì“°ê¸° ê¸°ë²• ì•ˆë‚´

### ì‚¬íšŒ (Social Studies)
- ì—­ì‚¬ì  ì‚¬ê±´ ë° ì¸ë¬¼ ì„¤ëª…
- ì§€ë¦¬ì  ê°œë… ë° í˜„ìƒ
- ì‚¬íšŒ ì œë„ ë° ë¬¸í™” ì´í•´

## ğŸ’» ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **GPU ë©”ëª¨ë¦¬**: ìµœì†Œ 16GB (ê¶Œì¥ 24GB+)
- **ì‹œìŠ¤í…œ RAM**: ìµœì†Œ 16GB
- **Python**: 3.8+
- **ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**: transformers, peft, torch

## âš ï¸ ì£¼ì˜ì‚¬í•­

1. **êµìœ¡ ëª©ì  íŠ¹í™”**: ì´ ëª¨ë¸ì€ êµìœ¡ ì½˜í…ì¸  ìƒì„±ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
2. **í•œêµ­ì–´ ì¤‘ì‹¬**: í•œêµ­ì–´ ì™¸ì˜ ì–¸ì–´ì—ì„œëŠ” ì„±ëŠ¥ì´ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **ì‚¬ì‹¤ í™•ì¸ í•„ìš”**: ìƒì„±ëœ ë‚´ìš©ì€ í•­ìƒ ê²€í† í•˜ê³  ì‚¬ì‹¤ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
4. **ìœ¤ë¦¬ì  ì‚¬ìš©**: êµìœ¡ì ì´ê³  ê±´ì „í•œ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

## ğŸ”— ê´€ë ¨ ë§í¬

- **ë² ì´ìŠ¤ ëª¨ë¸**: [unsloth/gpt-oss-20b](https://huggingface.co/unsloth/gpt-oss-20b)
- **ë°ì´í„°ì…‹**: [maywell/korean_textbooks](https://huggingface.co/datasets/maywell/korean_textbooks)

## ğŸ“œ ë¼ì´ì„ ìŠ¤

ì´ ëª¨ë¸ì€ ë² ì´ìŠ¤ ëª¨ë¸ì¸ unsloth/gpt-oss-20bì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
"""

    # README.md íŒŒì¼ ì €ì¥
    readme_path = os.path.join(model_path, "README.md")
    with open(readme_path, "w", encoding="utf-8") as f:
        f.write(readme_content)

    print("âœ… ëª¨ë¸ì¹´ë“œ(README.md) ìƒì„± ì™„ë£Œ")

def save_and_upload_model():
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ"""

    # 1. ëª¨ë¸ ì €ì¥
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    model_path = "./korean_textbook_model"

    try:
        # íŒŒì¸íŠœë‹ í›„ ì €ì¥
        model.save_pretrained(model_path)
        tokenizer.save_pretrained(model_path)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    except NameError:
        print("âŒ model ë˜ëŠ” tokenizer ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("íŒŒì¸íŠœë‹ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        return

    # 2. ì‚¬ìš©ì ì…ë ¥ (ëª¨ë¸ì¹´ë“œ ìƒì„±ì— í•„ìš”)
    repo_name = input("ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ (ì˜ˆ: PAUL1122/my-korean-model): ").strip()
    if not repo_name:
        print("âŒ ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    # 3. ëª¨ë¸ì¹´ë“œ ìƒì„±
    create_model_card(model_path, repo_name)

    # 4. ì„¤ì • íŒŒì¼ ìˆ˜ì •
    fix_adapter_config(model_path)

    # 5. í† í° ì…ë ¥
    token = input("Hugging Face í† í°: ").strip()
    if not token:
        print("âŒ í† í°ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    # 6. ì—…ë¡œë“œ
    try:
        print("ğŸš€ ì—…ë¡œë“œ ì‹œì‘...")

        # ë¡œê·¸ì¸
        login(token=token)
        api = HfApi(token=token)

        # ë¦¬í¬ì§€í† ë¦¬ ìƒì„±
        api.create_repo(repo_id=repo_name, exist_ok=True)
        print(f"âœ… ë¦¬í¬ì§€í† ë¦¬ ìƒì„±: {repo_name}")

        # í´ë” ì—…ë¡œë“œ
        api.upload_folder(
            folder_path=model_path,
            repo_id=repo_name,
            commit_message="Upload fine-tuned Korean model with model card"
        )

        print(f"ğŸ‰ ì—…ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ”— ë§í¬: https://huggingface.co/{repo_name}")

    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

def upload_existing_model():
    """ì´ë¯¸ ì €ì¥ëœ ëª¨ë¸ ì—…ë¡œë“œ"""

    # 1. ê¸°ë³¸ ì„¤ì •
    model_path = "./korean_textbook_model"

    # 2. ëª¨ë¸ í´ë” í™•ì¸
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("ë¨¼ì € save_and_upload_model() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    # 3. ì‚¬ìš©ì ì…ë ¥
    repo_name = input("ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ (ì˜ˆ: PAUL1122/my-korean-model): ").strip()
    if not repo_name:
        print("âŒ ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    # 4. ëª¨ë¸ì¹´ë“œ ìƒì„± (ì—†ê±°ë‚˜ ì—…ë°ì´íŠ¸)
    create_model_card(model_path, repo_name)

    # 5. ì„¤ì • íŒŒì¼ ìˆ˜ì •
    fix_adapter_config(model_path)

    # 6. í† í° ì…ë ¥
    token = input("Hugging Face í† í°: ").strip()
    if not token:
        print("âŒ í† í°ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return

    # 7. ì—…ë¡œë“œ
    try:
        print("ğŸš€ ì—…ë¡œë“œ ì‹œì‘...")

        # ë¡œê·¸ì¸
        login(token=token)
        api = HfApi(token=token)

        # ë¦¬í¬ì§€í† ë¦¬ ìƒì„±
        api.create_repo(repo_id=repo_name, exist_ok=True)
        print(f"âœ… ë¦¬í¬ì§€í† ë¦¬ ìƒì„±: {repo_name}")

        # í´ë” ì—…ë¡œë“œ
        api.upload_folder(
            folder_path=model_path,
            repo_id=repo_name,
            commit_message="Upload fine-tuned Korean model with model card"
        )

        print(f"ğŸ‰ ì—…ë¡œë“œ ì™„ë£Œ!")
        print(f"ğŸ”— ë§í¬: https://huggingface.co/{repo_name}")

    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì‚¬ìš©ë²• ì•ˆë‚´
print("ğŸ“– ì‚¬ìš© ë°©ë²•:")
print("1. íŒŒì¸íŠœë‹ ì§í›„ ì €ì¥+ì—…ë¡œë“œ: save_and_upload_model()")
print("2. ì´ë¯¸ ì €ì¥ëœ ëª¨ë¸ ì—…ë¡œë“œ: upload_existing_model()")
print()

# ì‹¤í–‰ ì„ íƒ
choice = input("ì„ íƒí•˜ì„¸ìš” (1: ì €ì¥+ì—…ë¡œë“œ, 2: ì—…ë¡œë“œë§Œ): ").strip()

if choice == "1":
    save_and_upload_model()
elif choice == "2":
    upload_existing_model()
else:
    print("ì˜¬ë°”ë¥¸ ì„ íƒì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (1 ë˜ëŠ” 2)")